​​My solution is a website that highlights some basic and some not-so-basic trends about the SFPD emergency dispatch dataset, and uses those insights to formulate solution steps to reduce response times. Using data ​analysis and visualization tools, I came up with the following three steps that I think should be taken to help improve dispatch efficiency. All are explained and displayed in more detail on the website.

1. ​​Finding route optimizations to and through commercial and urban areas, and conducting investigations of ways around mountainous terrain would help dispatchers reach some of the most frequently affected or highest average response time areas faster. 

​2. Enhancing marshalling​​ of late nighttime responders. 

3. Increasing awareness of alternate emergency services serving an area. 

On the "Future Preparation" page, I also identify some zipcodes that have proven to have particular emergencies with high frequency, and pinpoint key areas and locations for new dispatch services to cater to these high needs. 

My personal three Data Visual trends can be found on the homepage, and my steps for reducing response time for particular areas are explained in the "Dispatch Time" page. As for predicting dispatches, I created a small interactive part of the website under "Predicting Emergency", in which users can input a place (zipcode or station area) and specific time and view the top most likely emergencies from studying January. I also added a "Heatmaps" page as well as a "Crime Correlation" page showing some of the safest and least-safest areas.

This work could not have been possible without amazing and open source data science libraries, such as Jupyter Notebook, pandas, matplotlib, and seaborn, which powered all of my data analysis efforts and generated visualizations that I could use on the website. I created the website from scratch using HTML/CSS/JS and Bootstrap.  